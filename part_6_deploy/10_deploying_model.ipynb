{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mamy model i co dalej?\n",
    "\n",
    "Przygotowaliśmy dane, wybralismy cechy, wybraliśmy model - gotowe! No, nie do końca. Jest ML żeby wygrać w buzzwords bingo potrzebujemy jeszcze microservice'u! ;-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[328836.65397741]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sklearn.model_selection\n",
    "import sklearn.linear_model\n",
    "import sklearn.metrics\n",
    "import seaborn as sns\n",
    "import math\n",
    "%matplotlib inline  \n",
    "\n",
    "r = pd.read_csv('ceny_mieszkan_w_poznaniu.tsv', sep = '\\t')\n",
    "\n",
    "r_train, r_test = sklearn.model_selection.train_test_split(r, test_size = 0.2)\n",
    "model = sklearn.linear_model.LinearRegression()\n",
    "\n",
    "features = ['sqrMeters', 'rooms']\n",
    "label = ['price']\n",
    "\n",
    "model = sklearn.linear_model.LinearRegression()\n",
    "X_train = r_train[features]\n",
    "y_train = r_train[label].values.reshape(-1, 1)\n",
    "\n",
    "X_test = r_test[features]\n",
    "y_test = r_test[label].values.reshape(-1, 1)\n",
    "model.fit(X_train,y_train)\n",
    "\n",
    "sqr_meters = 71\n",
    "no_of_rooms = 2\n",
    "model.predict([[sqr_meters, no_of_rooms]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W przypadku sklearn'a można wyeksportować model w formie pliku *pickle*, który następnie można wczytać w miejscu gdzie chcemy z niego korzystać. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[328836.65397741]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# model zostanie zapisany w pliku pkl\n",
    "pickle.dump(model, open(\"model.pkl\",\"wb\"))\n",
    "\n",
    "# możemy go potem wczytać i wykorzystać do serwowania predykcji\n",
    "model_p = pickle.load(open(\"model.pkl\",\"rb\"))\n",
    "\n",
    "model_p.predict([[sqr_meters, no_of_rooms]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Przykład osadzenia takiego modelu w prostej aplikacji webowej możecie znaleźć w podkatalogu `ai_service`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[36m__pycache__\u001b[m\u001b[m   ai_service.py model.pkl     readme.txt\r\n"
     ]
    }
   ],
   "source": [
    "!ls ai_service/version_01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import pickle\r\n",
      "import flask\r\n",
      "from flask import request\r\n",
      "\r\n",
      "app = flask.Flask(__name__)\r\n",
      "\r\n",
      "#getting our trained model from a file we created earlier\r\n",
      "model = pickle.load(open(\"model.pkl\",\"rb\"))\r\n",
      "\r\n",
      "@app.route('/predict', methods=['POST'])\r\n",
      "def predict():\r\n",
      "    #grabbing a set of features from the request's body\r\n",
      "    feature_array = request.get_json()['feature_array']\r\n",
      "    \r\n",
      "    #our model rates flat based on the input array\r\n",
      "    prediction = model.predict([feature_array]).tolist()\r\n",
      "    \r\n",
      "    #preparing a response object and storing the model's predictions\r\n",
      "    response = {}\r\n",
      "    response['predictions'] = prediction\r\n",
      "    \r\n",
      "    #sending our response object back as json\r\n",
      "    return flask.jsonify(response)"
     ]
    }
   ],
   "source": [
    "!cat ai_service/version_01/ai_service.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Informacje o tym jak uruchomić aplikacje możecie znaleźć w pliku readme.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Install flask: \r\n",
      "pip3 install flask\r\n",
      "\r\n",
      "Launch the app: \r\n",
      "export FLASK_APP=ai_service.py; flask run\r\n",
      "\r\n",
      "Get predictions: \r\n",
      "curl 'http://localhost:5000/predict' -d '{\"feature_array\":[71,3]} ' -XPOST -H \"Content-type: application/json\"\r\n",
      "\r\n",
      "Should result in something like:\r\n",
      "{\"predictions\":[[419191.0334601513]]}\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!cat ai_service/version_01/readme.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipelines\n",
    "\n",
    "Przygotowując dane na potrzeby uczenia modelu dokonujemy różnych przekształceń (dodajemy one-hot'y, przemapowujemy cechy, wybieramy tylko niektóre kolumny). Ważne, jest żeby pamiętać, że analogiczne przeszktałcenia muszą zostać przeprowadzone w przypadku danych wejściowych na etapie predykcji. Z pesperktywy praktycznej najlepiej jak ten sam kod jest używany na etapie trenowania jak i predykcji. Sklearn udostępnia Pipelines, dzięki którym możemy zapisać sekwencje operacji w formie podobnej do bash'owego potoku ;-) \n",
    "\n",
    "np. wybierz_cechy | dodaj cechy one hot | policz_coś_tam | wytrenuj\n",
    "\n",
    "Przykładowy pipeline wygląda jak poniżej."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('feature selection', FeatureEnhancer()), ('regression', LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None,\n",
       "         normalize=False))])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from enhancer import FeatureEnhancer\n",
    "\n",
    "regr = sklearn.linear_model.LinearRegression()\n",
    "features = FeatureEnhancer()\n",
    "p = Pipeline([\n",
    "    ('feature selection', features),\n",
    "    ('regression', regr)\n",
    "])\n",
    "\n",
    "X_train = r_train\n",
    "y_train = r_train[label].values.reshape(-1, 1)\n",
    "\n",
    "p.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Każdy z kroków przetwarzania to tak naprawdę obiekt, który musi implementować metody fit i transform. Powyższe wywołanie metody p.fit() spowoduje odpalenie metod transform i fit na elementach w pip'ie.\n",
    "\n",
    "Poniżej przykład klasy FeatureEnhancer, który wybiera z podanego dataframe'a cechy istotne w naszym modelu i dodaje nową ```is_center```. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from sklearn.base import BaseEstimator, TransformerMixin\r\n",
      "\r\n",
      "class FeatureEnhancer(BaseEstimator):\r\n",
      "    def fit(self, X, y=None):\r\n",
      "        return self\r\n",
      "\r\n",
      "    def transform(self, X):      \r\n",
      "        \r\n",
      "        def get_is_center(district):\r\n",
      "            if district is \"Wilda\":\r\n",
      "                return 1\r\n",
      "            return 0\r\n",
      "        \r\n",
      "        out = X.copy()\r\n",
      "        out['is_center'] = X['location'].map(get_is_center)\r\n",
      "        return out[['sqrMeters', 'rooms','is_center']]"
     ]
    }
   ],
   "source": [
    "!cat enhancer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Przekształcenia wykonane na etapie uczenia zostaną również zaaplikowane na etapie predykcji."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[584076.05515689]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_df = pd.DataFrame({\n",
    "              'sqrMeters' : pd.Series([120]), \n",
    "              'rooms' : pd.Series([4]),\n",
    "              'location' : pd.Series(['Wilda'])\n",
    "            })\n",
    "\n",
    "p.predict(input_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tak wytrenowany model wraz z przekształceniami możemy zserializować i zapisać do wykorzystania w drugiej wersji naszej aplikacji."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_filename = \"pipe_with_module.pkl\"\n",
    "\n",
    "pickle.dump(p, open(model_filename,\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "piped = pickle.load(open(model_filename,\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[584076.05515689]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "piped.predict(input_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[36m__pycache__\u001b[m\u001b[m   ai_service.py model.pkl     readme.txt\r\n"
     ]
    }
   ],
   "source": [
    "!ls ai_service/version_01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ex. 1**\n",
    "\n",
    "Doprowadźcie drugą wersję ai_service do postaci działającej."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import pickle\r\n",
      "import flask\r\n",
      "from flask import request\r\n",
      "import pandas as pd\r\n",
      "from enhancer import FeatureEnhancer\r\n",
      "from sklearn.externals import joblib\r\n",
      "\r\n",
      "app = flask.Flask(__name__)\r\n",
      "\r\n",
      "#getting our trained model from a file we created earlier\r\n",
      "model = pickle.load(open(\"pipe_with_module.pkl\",\"rb\"))\r\n",
      "\r\n",
      "@app.route('/predict', methods=['POST'])\r\n",
      "def predict():\r\n",
      "    #grabbing a set of features from the request's body\r\n",
      "    input_dataframe = # construct dataframe from request.get_json()\r\n",
      "    \r\n",
      "    #our model predicts price of flat based on dataframe\r\n",
      "    prediction = model.predict(input_dataframe).tolist()\r\n",
      "    \r\n",
      "    #preparing a response object and storing the model's predictions\r\n",
      "    response = {}\r\n",
      "    response['prediction'] = prediction\r\n",
      "    \r\n",
      "    #sending our response object back as json\r\n",
      "    return flask.jsonify(response)\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!cat ai_service/version_02/ai_service.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Install flask: \r\n",
      "pip3 install flask\r\n",
      "\r\n",
      "Launch the app: \r\n",
      "export FLASK_APP=ai_service.py; flask run\r\n",
      "\r\n",
      "Get predictions: \r\n",
      "curl 'http://localhost:5000/predict' -d '{\"sqrMeters\":71, \"rooms\":2, \"location\":\"Wilda\"} ' -XPOST -H \"Content-type: application/json\"\r\n",
      "\r\n",
      "Should result in something like:\r\n",
      "{\"prediction\":[[419191.0334601513]]}\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!cat ai_service/version_02/readme.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
