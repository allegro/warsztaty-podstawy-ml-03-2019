{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Co to jest ml?\n",
    "\n",
    "## Standardowe programowanie\n",
    "\n",
    "W standardowym programowaniu piszemy algorytmy określące explicite zachowanie komputera. Na przykład jeżeli wnioskodawca zarabia powyżej 60 tys. złotych rocznie, ma stały dochód, to uważamy że można udzielić mu kredytu, w innym przypadku odrzucamy wniosek.\n",
    "\n",
    "Rodzą się pytania:\n",
    "* dlaczego akurat wybraliśmy próg 60 tys. złotych, a nie inny?\n",
    "* dlaczego uważamy że stały dochód jest dobrym predyktorem, może np. wnioskodawca jest godny zaufania, ale pracuje na kontraktach?\n",
    "* dlaczego bierzemy akurat te cechy, a nie inne?\n",
    "\n",
    "Często na te pytania odpowiadają eksperci dziedzinowi, czasem robi się to na czuja, czasem odpowiadają na nie statystycy (wykorzystujący statystyczne modele parametryczne zakładające np. rozkład normalny w populacji).\n",
    "\n",
    "## Machine learning (uczenie maszynowe)\n",
    "\n",
    "*ML* skupia się na tym, żeby stworzyć algorytm (np. model predykcyjny). Następnie podajemy dane do tego modelu - wtedy _maszyna się uczy_. Podczas procesu uczenia dzięki algorytmowi model sam dowiaduje się na jakich cechach polegać, jakie wybrać progi, itp. W zależności od tego jakie podamy dane modelowi, taki model otrzymamy.\n",
    "\n",
    "\n",
    "Dane mogą być historyczne, np. baza danych klientów, którzy brali kredyt w poprzednich latach:\n",
    "\n",
    "|id klienta| dochód roczny(tys) | stały dochód | wiek | ilość członków rodziny | czy spłacił kredyt|\n",
    "|----------|--------------------|--------------|------|------------------------|-------------------|\n",
    "| 1        | 50                 | tak          | 23   | 4                      | tak               |\n",
    "| 2        | 67                 | tak          | 29   | 3                      | nie               |\n",
    "| 3        | 102                | nie          | 45   | 4                      | tak               |\n",
    "| 4        | 64                 | tak          | 73   | 1                      | tak   \n",
    "\n",
    "\n",
    "**Jakość modelu ściśle zależy od danych.** Jeżeli dane są kiepskie (np. mamy ich bardzo mało, albo na skutek błędu podaliśmy, że osoby bez dochodu spłacają kredyt, podczas gdy nie jest to prawdą), to model będzie działał kiepsko.\n",
    "Algorytm machinelearningowy również musi być dobrze dopasowany do problemu.\n",
    "\n",
    "**dobry model ML** = dobry **algorytm** dopasowany do problemu **+** dobrej jakości **dane**\n",
    "\n",
    "## Uczenie maszynowe, a metody statystyczne\n",
    "\n",
    "![](ml_introduction_pictures/gosset_picture.jpg)\n",
    "\n",
    "\n",
    "Związek machine learningu i statystyki nie jest łatwy do opisania, nie ma też między tymi dziedzinami wyraźnej granicy. Wcześniej to głównie statystycy tworzyli modele oparte o dane. Działają oni często w oparciu o małe ilości danych (np. próba 100 była duża, obecnie kilka milionów). Zakładają oni prawdziwość pewnego modelu (np. rozkład normalny cechy w populacji, np. wzrost) i dobierają jedynie parametry na podstawie próby, takie jak wartość oczekiwana (średni wzrost) oraz odchylenie standardowe (jak bardzo ludzie różnią się wzrostem). Takie modele to statystyczne modele parametryczne, charakteryzują się one:\n",
    "\n",
    "* zakładają coś apriori, co nie musi być poprawne (np. rozkład normalny)\n",
    "* w związku z tym nie uwzględniają bardzo skomplikowanych zależności w danych (np. osoba zupełnie bez rodziny albo z ponad pięcioma członkami rodziny dobrze spłaca kredyt), model statystyczny może zakładać że im wiecej (mniej) członków rodziny, tym można ufać wnioskodawcy bardziej (mniej) )\n",
    "* dobrze działają na małej ilości danych\n",
    "* dają oszacowanie niepewności modelu (np. że kredytobiorca spłaci kredyt, ale jesteśmy tego pewni na 80%)\n",
    "* często dają wyjaśnienie modelu (np. dana cecha wpływa w taki, a taki sposób- ilość duża członków rodziny, powoduje że można ufać wnioskodawcy\n",
    "\n",
    "![](ml_introduction_pictures/gpu3.jpg)\n",
    "\n",
    "\n",
    "ML potrafi wykorzystać potencjał ogromnej ilości danych, znaleźć bardzo skomplikowane zależności, których nie da się wyrazić statystycznie. Wreszcie skupia się na wydajności algorytmów  dużej ilości danych. Niestety takie podejście często nie daje interpretacji wyników i nie daje oszacowania niepewności (często model dobrze trafia, że wnioskodawca spłaca kredyt zgodnie zgodnie z naszym modelem, ale model nie potrafi powiedzieć jak bardzo jest pewny swoich oszacowań). Czasami zdarza się, że algorytm daje świetne wyniki, ale nie wiemy dokładnie dlaczego (mamy heurystyki i wyjaśnienia, ale nie jesteśmy co do nich w 100% pewni)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Podstawowe zagadnienia ML\n",
    "\n",
    "## zagadnienia\n",
    "\n",
    "* supervised learning (uczenie z nauczycielem, uczenie nadzorowane)\n",
    " * klasyfikacja\n",
    " * regresja\n",
    "* unsupervised learning (uczenie bez nauczyciela)\n",
    " * Klasteryzacja (analiza skupień)\n",
    " * redukcja wymiarowości\n",
    "* reinforcement learning (uczenie przez wzmacnianie)\n",
    "\n",
    "## supervised learning\n",
    "\n",
    "Polega na tym, że dostarczamy modelowi obserwacje, które składają się ze zmiennych niezależnych (cechy opisujące obserwację- np. dane o wnioskodawcy) oraz zmiennej zależnej- etykiety (informację czy spłacił kredyt).\n",
    "\n",
    "Zakładamy że zmienna zależna (etykieta, zmienna objaśniana) zależy od zmiennnych niezależnych (zmiennych objaśniających)\n",
    "\n",
    "**obserwacja** = **zmienne niezależne** + **zmienna zależna**\n",
    "\n",
    "W przypadku kredytu:\n",
    "* zmiennie niezależne\n",
    " * dochód roczny(tys)\n",
    " * czy stały dochód\n",
    " * wiek\n",
    " * ilość członków rodziny\n",
    "* zmienna zależna\n",
    " * czy wnioskodawca spłacił kredyt\n",
    " \n",
    "\n",
    "Algortm na podstawie tych danych uczy się, jaki profil osób spłaca kredyt, a jaki nie.\n",
    "\n",
    "Po wyuczeniu (wytrenowaniu) modelu, mamy maszynkę, która na podstawie zmiennych niezależnych odpowiada na pytanie jak będzie wyglądać zmienna zależna.\n",
    "\n",
    "**Zmienne niezależne** -> model ml -> **zmienna zależna**\n",
    "\n",
    "W naszym przypadku podajemy modeli informację o wnioskodawcy: \n",
    "\n",
    " * dochód roczny (tys)\n",
    " * czy stały dochód\n",
    " * wiek\n",
    " * ilość członków rodziny\n",
    "\n",
    "A model mówi, czy ta osoba spłaci kredyt, czy nie spłaci.\n",
    "\n",
    "### Klasyfikacja\n",
    "\n",
    "![](ml_introduction_pictures/class1.jpg)\n",
    "\n",
    "\n",
    "Klasyfikacja przewiduje wartość dyskretną ze skończonego zbioru klas (jedno pole ze skończonej ilości), np:\n",
    "* czy obrazek przedstawia kota czy psa\n",
    "* czy osoba spłaci kredyt, czy nie\n",
    "* jaką cyfrę widzimy na obrazku (1,2,3...9)\n",
    "\n",
    "Klasyfikacja która ma przewidziec jedną z dwóch klas nazywamy klasyfikacją **binarną**\n",
    "\n",
    "Klasyfikacja która ma przewidziec jedną z więcej niżdwóch klas nazywamy klasyfikacją **wieloklasową**\n",
    "\n",
    "Ostatecznie algorytm albo ma rację (mówi, że osoba spłaci kredyt i rzeczywiście spłaciła lub że osoba nie spłaci i rzeczywiście nie spłaciła) lub nie ma racji (mówi że osoba spłaci kredyt, ale nie w rzeczywistości spłaciła lub że nie spłaci, ale rzeczywiście spłaciła).\n",
    "\n",
    "Miarą skuteczności algorytmu jest **dokładność** (accuracy) = ile razy algorytm miał rację / ile ogólnie było pytań\n",
    "\n",
    "Są też inne miary (metryki), ale o tym później.\n",
    "\n",
    "\n",
    "### Regresja\n",
    "\n",
    "![](ml_introduction_pictures/reg2.png)\n",
    "\n",
    "\n",
    "Regresja przewiduje wartość ciągłą, np.\n",
    "* cena mieszkania w Poznaniu\n",
    "* wzrost człowieka\n",
    "* Temperatura\n",
    "\n",
    "Zatem tutaj nigdy nie trafimy idealnie z estymacją (bo powiemy, że człowiek będzie mierzył 1,802342 cm, a w istocie będzie mierzył 1,802343 cm), co wynika ze specyfiki wartości ciągłej. Ale możemy mylić się mało ( o 0,000001 cm) lub bardzo dużo (30 cm)\n",
    "\n",
    "Miarą skuteczności algorytmu jest **RMSE** (root mean square error, błąd średnio-kwadratowy, czyli o jaką wartość się średnio pomyliliśmy:\n",
    "\n",
    "![](ml_introduction_pictures/rmsew.png)\n",
    "\n",
    "![](ml_introduction_pictures/residual.png)\n",
    "\n",
    "\n",
    "![Klasyfikacja, a regresja](ml_introduction_pictures/class_reg2.png)\n",
    "\n",
    "\n",
    "## Unsupervised learning\n",
    "\n",
    "W unsupervised learning na wejściu algorytm nie dostaje obserwacji z etykietami. Algorytm nie przewidzi nam, czy np. dana osoba spłaci kredyt, ale ma inne zastosowania.\n",
    "\n",
    "**obserwacja** = **zmienne**\n",
    "\n",
    "### Klasteryzacja\n",
    "\n",
    "![](ml_introduction_pictures/clasterization.png)\n",
    "\n",
    "\n",
    "Klasterycja (analiza skupień) pozwala na znalezienie skupień zbiorówości. Na przykład wśród użytkowników Spotify na podstawie ulubionych artystów użytkowników odnajdziemy kilka grup o podobnych preferencjach (ulubionych artystach).\n",
    "\n",
    "Analizując te skupienia ręcznie możemy dojść do wniosków, że skupienia opisują grupy:\n",
    "\n",
    "* fani muzyki klasycznej\n",
    "* fani metalu\n",
    "* fani electro\n",
    "* itp.\n",
    "\n",
    "Zastosowanie, to np. rekomendacje dla użytkowników Spotify, Netflixa, dopasowanie reklam, itp.\n",
    "\n",
    "### Redukcja wymiarowości\n",
    "\n",
    "![](ml_introduction_pictures/pca.png)\n",
    "\n",
    "Mając 2 zmienne łatwo narysować je na wykresie 2-wymiarowym (czasem nawet uda się narysowac 3 zmienne i trzecia zmienna jest kolorem punktu jeżeli to zmienna ze skończonego zbioru, dla 3 zmiennych ciągłych można rysować wykres trójwymiarowy). Mając 4 zmienne jest to raczej niemożliwe, nie mówiąc o 100 zmiennych.\n",
    "\n",
    "Redukcja wymiarowości m.in. wyszukuje najistotniejsze zmienne lub grupuje wiele zmiennych w jedną (w bardzo dużym, nieprezycyjnym uproszczeniu).\n",
    "\n",
    "Mniejsza ilość wymiarów pozwala na wizualizację i szybsze działanie algorytmów, przy małej utracie dokładności."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zbiór trenujący i testowy\n",
    "\n",
    "Wyobraźmy sobie sytuację: Przykładowy student, który uczy się na egzamin. Ma od kolegi pytania z odpowiedziami (obserwacje z etykietami). Jeżeli wykuje się na pamięć pytań z odpowiedziami, będzie potrafił odpowiedzieć na te konkretne pytania bardzo dobrze i dostanie piątkę, jeżeli te konkretne pytania będą na egzaminie. Oczywiście na egzaminie będą inne pytania i wtedy student dostanie inną ocenę. Jeżeli nauczy się wiedzy ogólnej na temat zagadnienia, to nawet jeżeli będzie mylił się czasami na zbiorze pytań z odpowiedziami do nauki, to na egzaminie z nowymi pytaniami pójdzie mu dobrze.\n",
    "\n",
    "Podobnie rzecz ma się z modelami supervised learning ml. Jesteśmy w stanie zrobić model który działa perfekcyjnie na danych, na których go uczymy. Natomiast ten **model niekoniecznie będzie dobrze sprawował się na nowych danych** (jeżeli nawet tylko trochę będą się różnić).\n",
    "\n",
    "Dlatego własnie w ML korzystamy z 2 zbiórów:\n",
    "* uczącego - służącego do nauki modelu (zmienne niezależne + etykieta)\n",
    "* testowego - służącego do ewaluacji modelu (zmienne zależne, model stara się odgadnać etykiete i sprawdzamy czy ma rację)\n",
    "\n",
    "Pozwala to zapobiec problemowi nadmiernego dopasowania (**overfitting- model świetnie działa na danych, na których się uczy, a kiepsko na nowych danych, których nie widział**). Ostatecznie chcemy mieć model, który dobrze przewidzi czy nowa osoba jest w stanie spłacić kredyt. Najbardziej interesuje nas wynik na nowych danych.\n",
    "\n",
    "![](ml_introduction_pictures/overfitting.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cykl pracy nad MLem\n",
    "\n",
    "Droga modelu MLowego do produkcyjnej usługi składa się z reguły z kilku kroków:\n",
    "* **Eksploracja danych** \n",
    "    * identyfikacja braków w danych \n",
    "    * poszukiwanie potencjalnych korelacji między cechami, a funkcją celu\n",
    "* **Inżynieria cech** (ang. feature engineering) \n",
    "    * czyszczenie danych odstających (ang. outliers)\n",
    "    * normalizacja\n",
    "    * łączenie zbioru danych ze źródłami zewnętrznymi (np. pogoda, dzień tygodnia)\n",
    "    * łączenie cech ze sobą (ang. feature crosses - np. szerokość i długość geograficzna)\n",
    "    * różnego rodzaju przeształcenia (bucketzacja - zmiana cechy ciągłej na kategoryczną)\n",
    "* **Stworzenie modelu** \n",
    "    * Wybranie algorytmu\n",
    "    * Uczenie modelu\n",
    "    * Weryfikacja wyników\n",
    "    * Tuning hiperparametrów\n",
    "* **Deployment produkcyjny**\n",
    "    * w zależności od technologii \n",
    "* **Monitoring produkcyjny**\n",
    "    * Weryfikacja czy rozkłady wartości cech które są podstawą dla działania modelu są wciąż aktualne.\n",
    "\n",
    "Przeważnie tworzenie modelu to wisienka (20% czasu) na torcie, a prawdziwa brudna/trudna/żmudna robota to przygotowanie i zrozumienie danych (80% czasu).  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML w Allegro\n",
    "\n",
    "Cały czas się uczymy, ale jest już kilka rzeczy które śmigają na prodzie:\n",
    "* Ocena obrazków z ofert allegrowych pod kątem zgodności z wymaganiami (białe tło, brak ramki, brak napisów)\n",
    "    * Technologicznie: sieci neuronowe, tensorflow, tensorflow-serving, docker, google cloud platform\n",
    "* Reranking wyników trafności, model Learning2Rank który \"poprawia\" trafność\n",
    "    * Technologicznie: usługa napisana w scali, wykorzystująca bibliotekę xgboost\n",
    "* Modele wykorzystywane w serwowaniu rekomendacji\n",
    "    * Technologicznie: własne narzędzia\n",
    "* Modele językowe wykorzystywane do poprawy błędnie wpisanych fraz\n",
    "    * Technologicznie: własne narzędzia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Wybór frameworka\" :-)\n",
    "\n",
    "Na etapie prototypowania i nauki często sięgamy po [scikit-learna](http://scikit-learn.org/stable/index.html), dla którego można znaleźć masę przykładów/dokumentacji w sieci. \n",
    "\n",
    "Żeby pracować z naszymi danymi i sięgać po zbiory danych które nie mieszczą się w pamięci można wykorzystać **PySparka** na naszym firmowym [Jupyterze](http://jupyter.allegrogroup.com). \n",
    "\n",
    "Python świetnie nadaje się do prototypowania i nauki, ale rozwiązania produkcyjne najczęściej tworzymy przy użyciu Scali i oryginalnego Sparka (jeżeli chodzi o przygotowanie danych dla modelu).\n",
    "\n",
    "W Sparku dostępna jest bibliotek **mlib**.\n",
    "\n",
    "Modele produkcyjne liczone są przy użyciu **Tensorflow**, a serwowane przy wykorzystaniu tensorflow-serving. W przypadku większych zbiorów danych fajnie sprawdziło się trenonwanie w **MLEngine** na **Google Cloud Platform**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
